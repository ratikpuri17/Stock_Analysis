{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "# spacy.load('en')\n",
    "# from spacy.lang.en import English\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = nlp(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('URL')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=word_tokenize(\"I love mango\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "\n",
    "    en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "    stop_list = [\"Mrs.\",\"Ms.\",\"say\",\"WASHINGTON\",\"'s\",\"Mr.\",\"@yahoofinance\",\"URL\"]\n",
    "\n",
    "    for w in stop_list:\n",
    "        en_stop.add(w)\n",
    "\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = os.environ['TWITTER_CONSUMER_KEY']\n",
    "consumer_secret = os.environ['TWITTER_CONSUMER_SECRET']\n",
    "access_token = os.environ['TWITTER_ACCESS_TOKEN']\n",
    "access_secret = os.environ['TWITTER_ACCESS_SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_trend():\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_secret)\n",
    "\n",
    "    api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())\n",
    "    tweets = api.user_timeline(screen_name='YahooFinance', count=120)\n",
    "    #json.dumps(status)\n",
    "\n",
    "    t=[]\n",
    "    for tweet in tweets:\n",
    "\n",
    "        t.append(tweet['text'])\n",
    "\n",
    "\n",
    "    tweet_data=[]\n",
    "\n",
    "    for tweet in t:\n",
    "        tokens = prepare_text_for_lda(tweet)\n",
    "        if random.random() > .99:\n",
    "            print(tokens)\n",
    "        tweet_data.append(tokens)\n",
    "\n",
    "\n",
    "    dictionary = corpora.Dictionary(tweet_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tweet_data]\n",
    "\n",
    "    NUM_TOPICS = 10\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "    # ldamodel.save('model5.gensim')\n",
    "    topics = ldamodel.print_topics(num_words=20)\n",
    "    # for topic in topics:\n",
    "    #     print(topic)\n",
    "\n",
    "    return all_words(ldamodel)\n",
    "\n",
    "def all_words(ldamodel):\n",
    "\n",
    "    l=ldamodel.show_topics(num_topics=10, num_words=20, log=False, formatted=False)\n",
    "#     print(len(l))\n",
    "#     print(l)\n",
    "\n",
    "    all_words=[]\n",
    "\n",
    "    for topic in range(len(l)):\n",
    "        \n",
    "        words_array=l[topic][1]\n",
    "        \n",
    "        for words in words_array:\n",
    "            all_words.append(words[0])\n",
    "        \n",
    "        \n",
    "\n",
    "    s=set(all_words)\n",
    "    \n",
    "    g=list(s)\n",
    "\n",
    "\n",
    "    return Word_Cloud(g)\n",
    "\n",
    "\n",
    "def Word_Cloud(l):\n",
    "    long_string = ','.join(l)\n",
    " \n",
    "    # Create a WordCloud object\n",
    "    wordcloud = WordCloud(background_color=\"white\",height=700,width=700, max_words=5000, contour_width=3, contour_color='steelblue')\n",
    "    # Generate a word cloud\n",
    "    wordcloud.generate(long_string)\n",
    "    # Visualize the word cloud\n",
    "    wordcloud.to_image()\n",
    "\n",
    "    return wordcloud\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['highlight', 'expect', 'expect', 'really', 'numbers', 'parcel', 'happen']\n",
      "['highlight', 'think', 'senate', 'going', 'accept', 'number', 'things', 'hero']\n",
      "['mortgage', 'refinance', 'activity', 'slide', 'rates']\n"
     ]
    }
   ],
   "source": [
    "a=fetch_trend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x19feea4c3c8>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.to_file(os.getcwd()+'\\\\static\\\\cloud.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
